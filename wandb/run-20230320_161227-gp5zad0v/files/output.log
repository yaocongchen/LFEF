
  0%|                                                                                                                                                                                  | 0/150 [00:00<?, ?it/s]
----- Training - Epoch 01 - batch_size 2,------
=====> the number of iterations per epoch:  64
=====> epoch[1/150] train_iter: (10/64) 	 loss: 0.990078 acc:0.384365
=====> epoch[1/150] train_iter: (20/64) 	 loss: 1.118121 acc:0.226176
=====> epoch[1/150] train_iter: (30/64) 	 loss: 1.205016 acc:0.281783
=====> epoch[1/150] train_iter: (40/64) 	 loss: 0.997592 acc:0.253877
=====> epoch[1/150] train_iter: (50/64) 	 loss: 0.834428 acc:0.384163

  1%|█▏                                                                                                                                                                        | 1/150 [00:04<11:28,  4.62s/it]
----- Training - Epoch 02 - batch_size 2,------
=====> the number of iterations per epoch:  64
=====> epoch[2/150] train_iter: (10/64) 	 loss: 1.038095 acc:0.366576
=====> epoch[2/150] train_iter: (20/64) 	 loss: 1.089909 acc:0.206481
=====> epoch[2/150] train_iter: (30/64) 	 loss: 0.937590 acc:0.332285
=====> epoch[2/150] train_iter: (40/64) 	 loss: 1.067490 acc:0.207169
=====> epoch[2/150] train_iter: (50/64) 	 loss: 1.070724 acc:0.357918

  1%|██▎                                                                                                                                                                       | 2/150 [00:08<10:14,  4.16s/it]
----- Training - Epoch 03 - batch_size 2,------
=====> the number of iterations per epoch:  64
=====> epoch[3/150] train_iter: (10/64) 	 loss: 1.104303 acc:0.304740
=====> epoch[3/150] train_iter: (20/64) 	 loss: 0.768188 acc:0.520617
=====> epoch[3/150] train_iter: (30/64) 	 loss: 0.876310 acc:0.412106
=====> epoch[3/150] train_iter: (40/64) 	 loss: 0.723787 acc:0.505397
=====> epoch[3/150] train_iter: (50/64) 	 loss: 1.061140 acc:0.292400
=====> epoch[3/150] train_iter: (60/64) 	 loss: 1.081111 acc:0.340341
----- Training - Epoch 04 - batch_size 2,------

  2%|███▍                                                                                                                                                                      | 3/150 [00:12<09:43,  3.97s/it]
=====> epoch[4/150] train_iter: (10/64) 	 loss: 0.828970 acc:0.336047
=====> epoch[4/150] train_iter: (20/64) 	 loss: 0.803012 acc:0.420126
=====> epoch[4/150] train_iter: (30/64) 	 loss: 0.947659 acc:0.514937
=====> epoch[4/150] train_iter: (40/64) 	 loss: 0.866647 acc:0.402974
=====> epoch[4/150] train_iter: (50/64) 	 loss: 0.865717 acc:0.498118
=====> epoch[4/150] train_iter: (60/64) 	 loss: 0.774281 acc:0.378223
----- Training - Epoch 05 - batch_size 2,------

  3%|████▌                                                                                                                                                                     | 4/150 [00:16<09:31,  3.91s/it]
=====> epoch[5/150] train_iter: (10/64) 	 loss: 0.851871 acc:0.516519
=====> epoch[5/150] train_iter: (20/64) 	 loss: 0.806053 acc:0.560393
=====> epoch[5/150] train_iter: (30/64) 	 loss: 0.730923 acc:0.557355
=====> epoch[5/150] train_iter: (40/64) 	 loss: 1.123550 acc:0.097462
=====> epoch[5/150] train_iter: (50/64) 	 loss: 1.172779 acc:0.253346
=====> epoch[5/150] train_iter: (60/64) 	 loss: 0.884962 acc:0.380090
----- Training - Epoch 06 - batch_size 2,------

  3%|█████▋                                                                                                                                                                    | 5/150 [00:19<09:20,  3.87s/it]
=====> epoch[6/150] train_iter: (10/64) 	 loss: 0.996671 acc:0.484181
=====> epoch[6/150] train_iter: (20/64) 	 loss: 0.842386 acc:0.454547
=====> epoch[6/150] train_iter: (30/64) 	 loss: 0.748938 acc:0.489874

  4%|██████▊                                                                                                                                                                   | 6/150 [00:23<09:14,  3.85s/it]
=====> epoch[6/150] train_iter: (50/64) 	 loss: 0.913261 acc:0.471701
=====> epoch[6/150] train_iter: (60/64) 	 loss: 0.841766 acc:0.368621
----- Training - Epoch 07 - batch_size 2,------
=====> the number of iterations per epoch:  64
=====> epoch[7/150] train_iter: (10/64) 	 loss: 0.824791 acc:0.392522
=====> epoch[7/150] train_iter: (20/64) 	 loss: 0.803988 acc:0.443440
=====> epoch[7/150] train_iter: (30/64) 	 loss: 0.735848 acc:0.462814

  5%|███████▉                                                                                                                                                                  | 7/150 [00:27<09:07,  3.83s/it]
=====> epoch[7/150] train_iter: (50/64) 	 loss: 0.802124 acc:0.410294
=====> epoch[7/150] train_iter: (60/64) 	 loss: 0.822068 acc:0.336499
----- Training - Epoch 08 - batch_size 2,------
=====> the number of iterations per epoch:  64
=====> epoch[8/150] train_iter: (10/64) 	 loss: 0.807601 acc:0.476814
=====> epoch[8/150] train_iter: (20/64) 	 loss: 0.756646 acc:0.468879
=====> epoch[8/150] train_iter: (30/64) 	 loss: 0.722441 acc:0.621025
=====> epoch[8/150] train_iter: (40/64) 	 loss: 0.735167 acc:0.432587

  5%|█████████                                                                                                                                                                 | 8/150 [00:31<09:04,  3.84s/it]
=====> epoch[8/150] train_iter: (60/64) 	 loss: 0.773168 acc:0.398280
----- Training - Epoch 09 - batch_size 2,------
=====> the number of iterations per epoch:  64
  5%|█████████                                                                                                                                                                 | 8/150 [00:33<09:54,  4.19s/it]
Traceback (most recent call last):
  File "/home/yaocong/Experimental/light_ssd/train.py", line 263, in <module>
    train(args)
  File "/home/yaocong/Experimental/light_ssd/train.py", line 157, in train
    loss.backward()
  File "/home/yaocong/miniconda3/envs/torch/lib/python3.9/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/yaocong/miniconda3/envs/torch/lib/python3.9/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
=====> epoch[9/150] train_iter: (20/64) 	 loss: 0.630098 acc:0.550498
=====> epoch[9/150] train_iter: (30/64) 	 loss: 0.766219 acc:0.411762