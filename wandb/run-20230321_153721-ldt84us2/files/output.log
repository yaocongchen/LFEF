----- Training - Epoch 01 - batch_size 2,------
=====> the number of iterations per epoch:  64
----- Training - Epoch 02 - batch_size 2,------
=====> the number of iterations per epoch:  64
----- Training - Epoch 03 - batch_size 2,------
=====> the number of iterations per epoch:  64
----- Training - Epoch 04 - batch_size 2,------
=====> the number of iterations per epoch:  64
----- Training - Epoch 05 - batch_size 2,------
=====> the number of iterations per epoch:  64
Traceback (most recent call last):
  File "/home/yaocong/Experimental/light_ssd/train.py", line 264, in <module>
    train(args)
  File "/home/yaocong/Experimental/light_ssd/train.py", line 160, in train
    optimizer.step()
  File "/home/yaocong/miniconda3/envs/torch/lib/python3.9/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "/home/yaocong/miniconda3/envs/torch/lib/python3.9/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/yaocong/miniconda3/envs/torch/lib/python3.9/site-packages/torch/optim/adam.py", line 234, in step
    adam(params_with_grad,
  File "/home/yaocong/miniconda3/envs/torch/lib/python3.9/site-packages/torch/optim/adam.py", line 300, in adam
    func(params,
  File "/home/yaocong/miniconda3/envs/torch/lib/python3.9/site-packages/torch/optim/adam.py", line 363, in _single_tensor_adam
    exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
KeyboardInterrupt